# Transformer Path Planner for Grid Environments

This project implements a Transformer-based model for pathfinding on 2D grid environments (similar to A*). It learns a planning policy by imitating expert trajectories generated by the A* algorithm. The core approach uses a local patch around the agent's current position as input to the Transformer, making it scalable to larger grid sizes (e.g., 100x100).

## Overview

Pathfinding algorithms like A* are optimal but can be computationally expensive due to the number of nodes explored, especially in large, complex environments. This project explores using a Transformer model to learn a direct mapping from the local environment state (a patch of the grid, current position, goal position) to the next best action, aiming to reduce the number of steps/nodes evaluated during planning.

The model is trained via supervised learning (behavioral cloning) on data generated by running A* on various grid configurations (random obstacles and mazes).

## Features

*   **Transformer-based Planner:** Utilizes a Transformer encoder architecture.
*   **Local Patch Input:** Processes an N x N patch around the agent, enabling scalability to large grids (e.g., 100x100) by avoiding quadratic complexity w.r.t. the full grid size.
*   **A* Expert Training Data:** Learns from optimal paths generated by A*.
*   **Configurable Environment:** Supports random obstacle grids and maze generation with adjustable parameters.
*   **A* Baseline Comparison:** Includes evaluation scripts to compare the Transformer planner's performance (success rate, path length, planning time/steps) against the A* algorithm.
*   **Configurable Parameters:** Model dimensions, training settings, grid sizes, patch sizes, etc., are configurable.
*   **Slurm Support:** Example shell scripts provided for running data generation, training, and evaluation on Slurm-based HPC clusters.

## Requirements

*   **Python 3.8+** (Python 3.6 might work, but 3.8+ is recommended; **Python 2.7 is NOT supported**)
*   **PyTorch** (tested with 1.10+, newer versions recommended)
*   **NumPy**
*   **Matplotlib** (for plotting results)
*   **tqdm** (for progress bars)
*   **(Optional) Optuna:** If you want to perform hyperparameter tuning (`pip install optuna`).

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd pathfinding-transformer
    ```

2.  **Set up a virtual environment (Recommended):**
    *   Using `venv`:
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        ```
    *   Using `conda`:
        ```bash
        conda create -n path_planner python=3.9 # Or your preferred 3.x version
        conda activate path_planner
        ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The project is structured around several key scripts. Example Slurm submission scripts (`run_*.sh`) are provided.

1.  **Generate Expert Data (`data_generation.py`):**
    *   This script uses the A* algorithm to generate state-action pairs on random grids or mazes.
    *   The generated data (list of tuples) is saved as a `.pkl` file.
    *   Configuration is primarily done in `config.py` (e.g., `NUM_TRAIN_TRAJECTORIES`, `TARGET_GRID_SIZE`).
    *   Run using the example Slurm script or directly:
        ```bash
        # Using Slurm (adjust resources in the script)
        sbatch run_generate_data.sh

        # Or directly
        python data_generation.py
        ```
    *   Output files will be saved in the `expert_data/` directory (defined in `config.py`).

2.  **Train the Model (`train.py`):**
    *   This script loads the generated expert data, initializes the Transformer model, and trains it.
    *   Hyperparameters and settings can be adjusted via command-line arguments (see `python train.py --help`) or by modifying `config.py`.
    *   Checkpoints and the best model (based on validation loss) are saved in the `trained_models/` directory. Training plots are also saved.
    *   Run using the example Slurm script or directly:
        ```bash
        # Using Slurm (adjust resources/GPU request in the script)
        sbatch run_training.sh

        # Or directly (example overriding some defaults)
        python train.py --epochs 50 --batch_size 128 --lr 1e-4 --patch_size 11 --grid_size 100
        ```
    *   Add the `--resume` flag to continue training from the latest checkpoint.

3.  **Evaluate the Planner (`evaluate.py`):**
    *   This script loads a trained model and evaluates its performance on newly generated test environments (random or maze).
    *   It compares the Transformer planner against the A* baseline.
    *   Specify the model to load using `--model_path`. Ensure `--grid_size` and `--patch_size` match the trained model.
    *   Evaluation metrics and plots are saved in the `evaluation_results/` directory.
    *   Run using the example Slurm script or directly:
        ```bash
        # Using Slurm
        sbatch run_evaluation.sh

        # Or directly (evaluating the default best model on 100x100 mazes)
        python evaluate.py --num_cases 200 --grid_size 100 --patch_size 11 --model_path trained_models/model_best.pth --use_maze
        ```

4.  **Hyperparameter Tuning (`hpt.py` - *Optional*):**
    *   If you create an `hpt.py` script using Optuna (based on examples in `train.py` comments), you can use it to search for optimal hyperparameters.
    *   Requires the `optuna` library (`pip install optuna`).
    *   An example `run_hpt.sh` would execute this script.

## File Structure

pathfinding-transformer/
├── requirements.txt # Python dependencies
├── config.py # Central configuration for parameters
├── environment.py # GridEnvironment class (grid generation, obstacles, etc.)
├── astar.py # A* search algorithm implementation
├── utils.py # Utility functions (actions, patches, plotting)
├── data_generation.py # Script to generate A* expert trajectories
├── dataset.py # PyTorch Dataset class for loading patch-based data
├── model.py # Transformer model definition (PathfindingTransformer)
├── train.py # Script for training the Transformer model
├── planner.py # TransformerPlanner class using the trained model
├── evaluate.py # Script for evaluating planner performance vs A*
├── run_generate_data.sh # Example Slurm script for data generation
├── run_training.sh # Example Slurm script for training
├── run_evaluation.sh # Example Slurm script for evaluation
├── expert_data/ # Directory for storing generated .pkl data (created by script)
├── trained_models/ # Directory for saving model checkpoints/best model (created by script)
├── evaluation_results/ # Directory for saving evaluation metrics/plots (created by script)
└── logs/ # Directory for storing Slurm log files (created by script)
└── (Optional) hpt.py # Script for hyperparameter optimization
└── (Optional) run_hpt.sh # Example Slurm script for HPT


## Configuration

Many key parameters are defined in `config.py`. Some can be overridden by command-line arguments in `train.py` and `evaluate.py`. Important parameters include:

*   `TARGET_GRID_SIZE`: The dimension of the square grids used.
*   `PATCH_SIZE`: The dimension of the local patch input to the Transformer (must be odd).
*   `EMBED_DIM`, `NUM_HEADS`, `NUM_LAYERS`, `D_FF`: Transformer architecture dimensions.
*   `LEARNING_RATE`, `BATCH_SIZE`, `NUM_EPOCHS`: Training hyperparameters.
*   `NUM_TRAIN_TRAJECTORIES`, `NUM_VAL_TRAJECTORIES`, `NUM_TEST_ENVIRONMENTS`: Dataset sizes.

## Future Work / Improvements

*   Experiment with different relative/absolute goal representations.
*   Explore alternative pooling methods or using specific token outputs (e.g., CLS token if added) in the Transformer.
*   Implement more sophisticated planning logic in `planner.py` (e.g., beam search, improved cycle detection, backtracking).
*   Investigate using Convolutional Neural Networks (CNNs) as feature extractors for the grid/patch.
*   Train on more diverse or challenging environment types.
*   Explore reinforcement learning approaches instead of behavioral cloning.

## License

This project is licensed under the [MIT] License - see the LICENSE file for details.
